name: CI

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

on:
  pull_request:
    branches: [master, main]
  workflow_dispatch:

permissions:
  contents: read

jobs:
  # Build job gates all tests - nothing runs if build fails
  build:
    name: Build
    runs-on: ubuntu-24.04
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2

      - name: Set up Go
        uses: actions/setup-go@7a3fe6cf4cb3a834922a1244abfce67bcef6a0c5 # v6.2.0
        with:
          go-version: '1.24'
          cache: true

      - name: Run golangci-lint
        uses: golangci/golangci-lint-action@1e7e51e771db61008b38414a730f564565cf7c20 # v9.2.0
        with:
          version: latest
          args: --timeout=5m

      - name: Build
        run: go build -v ./...

  # Go unit tests run in parallel with integration tests (after build passes)
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-24.04
    needs: build
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2

      - name: Set up Go
        uses: actions/setup-go@7a3fe6cf4cb3a834922a1244abfce67bcef6a0c5 # v6.2.0
        with:
          go-version: '1.24'
          cache: true

      - name: Run unit tests with coverage and race detector
        run: |
          go test -v -race -coverprofile=coverage.out -covermode=atomic ./...
          go tool cover -func=coverage.out | tail -1  # Show total coverage

  # Integration tests run in parallel with unit tests (after build passes)
  # Matrix runs tests in both OVN and bridge-only environments
  # Split into 6 test groups for parallel execution:
  #   - shell-ephemeral (15 tests) × 2 network types = 2 jobs
  #   - shell-persistent (9 tests) × 2 network types = 2 jobs
  #   - network (10 tests) × OVN only = 1 job
  #   - container-file (54 tests) × 2 network types = 2 jobs
  #   - core (83 tests) × 2 network types = 2 jobs
  #   - misc (70 tests) × 2 network types = 2 jobs
  # Total: 11 parallel jobs instead of 2 sequential
  # Result: ~15-20min total (down from 37min), 2x faster CI feedback
  integration:
    name: Integration Tests (${{ matrix.test_group.name }}, ${{ matrix.network_type }})
    runs-on: ubuntu-24.04
    needs: build
    timeout-minutes: 60
    strategy:
      fail-fast: false
      matrix:
        network_type:
          - ovn      # Full OVN networking with ACL support
          - bridge   # Standard bridge without OVN (tests error handling)
        test_group:
          - name: shell-ephemeral
            path: tests/shell/ephemeral
            description: "Ephemeral shell session tests (15 tests)"
          - name: shell-persistent
            path: tests/shell/persistent
            description: "Persistent shell session tests (9 tests)"
          - name: network
            path: tests/network
            description: "Network isolation tests (10 tests, OVN only)"
            ovn_only: true
          - name: container-file
            path: tests/container tests/file
            description: "Container and file operations (54 tests)"
          - name: core
            path: tests/list tests/attach tests/tmux tests/kill tests/run tests/persist tests/build
            description: "Core commands: list/attach/tmux/kill/run/persist/build (83 tests)"
          - name: misc
            path: tests/clean tests/completion tests/docker tests/errors tests/help tests/image tests/info tests/mount tests/shutdown tests/version tests/meta tests/main_help_flag.py tests/main_help_shorthand.py
            description: "Misc commands: clean/completion/docker/errors/help/image/info/mount/shutdown/version/meta/main help (70 tests)"
        exclude:
          # Network tests only run on OVN (they test OVN-specific ACL features)
          - network_type: bridge
            test_group:
              name: network
              path: tests/network
              description: "Network isolation tests (10 tests, OVN only)"
              ovn_only: true
    steps:
      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2

      - name: Set up Go
        uses: actions/setup-go@7a3fe6cf4cb3a834922a1244abfce67bcef6a0c5 # v6.2.0
        with:
          go-version: '1.24'
          cache: true

      - name: Set up Python
        uses: actions/setup-python@a309ff8b426b58ec0e2a45f0f869d46889d02405 # v6.2.0
        with:
          python-version: '3.12'

      - name: Cache Python dependencies
        uses: actions/cache@8b402f58fbc84540c8b491a91e594a4576fec3d7 # v5.0.2
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt', '**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r tests/support/requirements.txt

      - name: Lint Python code with ruff
        run: |
          echo "Running ruff linter on Python test code..."
          ruff check tests/
          echo "Checking Python code formatting..."
          ruff format --check tests/

      - name: Install Incus and OVN
        run: |
          # Install from Ubuntu's official repositories
          echo "Installing Incus from Ubuntu 24.04 repositories..."
          sudo apt-get update

          if [ "${{ matrix.network_type }}" = "ovn" ]; then
            sudo apt-get install -y incus ovn-host ovn-central
          else
            sudo apt-get install -y incus
          fi

          incus version
          echo "✓ Incus installed"

      - name: Configure OVN (deb packages)
        if: matrix.network_type == 'ovn'
        run: |
          # Configure OVS to connect to OVN (like Canonical lxd-ci does)
          # Do NOT set network.ovn.northbound_connection - let Incus auto-detect
          sudo ovs-vsctl set open_vswitch . \
            external_ids:ovn-remote=unix:/var/run/ovn/ovnsb_db.sock \
            external_ids:ovn-encap-type=geneve \
            external_ids:ovn-encap-ip=127.0.0.1

          echo "OVS configured:"
          sudo ovs-vsctl get open_vswitch . external_ids

      - name: Initialize Incus (OVN)
        if: matrix.network_type == 'ovn'
        run: |
          # Configure subuid/subgid BEFORE first Incus start
          echo "root:1001:1" | sudo tee -a /etc/subuid
          echo "root:1001:1" | sudo tee -a /etc/subgid

          # Initialize with just bridge (no OVN yet) - avoid OVS validation
          cat <<EOF | sudo incus admin init --preseed
          config:
            images.compression_algorithm: none
          networks:
          - config:
              ipv4.address: 10.47.62.1/24
              ipv4.nat: "true"
              ipv4.dhcp.ranges: 10.47.62.2-10.47.62.99
              ipv4.ovn.ranges: 10.47.62.100-10.47.62.200
              ipv6.address: none
            name: incusbr0
            type: bridge
          storage_pools:
          - config:
              size: 15GiB
            name: default
            driver: btrfs
          profiles:
          - config: {}
            devices:
              eth0:
                name: eth0
                network: incusbr0
                type: nic
              root:
                path: /
                pool: default
                type: disk
            name: default
          EOF

          sudo chmod 666 /var/lib/incus/unix.socket
          sudo usermod -aG incus-admin $USER

          echo "✓ Incus initialized with bridge"
          incus network list

      - name: Add OVN network
        if: matrix.network_type == 'ovn'
        run: |
          # Fix socket permissions so Incus can access them
          echo "=== Fixing socket permissions ==="
          sudo chmod 666 /run/openvswitch/db.sock
          sudo chmod 666 /var/run/ovn/ovnnb_db.sock
          sudo chmod 666 /var/run/ovn/ovnsb_db.sock

          # Debug: Check OVS and OVN sockets
          echo "=== Checking sockets ==="
          ls -la /run/openvswitch/
          ls -la /var/run/ovn/
          sudo ovs-vsctl show
          sudo ovn-nbctl show

          # Restart Incus so it picks up OVS/OVN
          echo "Restarting Incus to detect OVS/OVN..."
          sudo systemctl restart incus
          sleep 5

          # Set OVN connection explicitly via unix socket
          echo "Setting OVN northbound connection..."
          incus config set network.ovn.northbound_connection unix:/var/run/ovn/ovnnb_db.sock

          # Verify the setting was applied
          echo "Verifying OVN connection setting..."
          incus config get network.ovn.northbound_connection

          echo "Creating OVN network..."
          incus network create ovn-net --type=ovn \
            network=incusbr0 \
            bridge.mtu=1442 \
            ipv4.address=10.215.220.1/24 \
            ipv4.nat=true \
            ipv6.address=none

          # Update default profile to use OVN network
          incus profile device remove default eth0
          incus profile device add default eth0 nic network=ovn-net

          echo "✓ OVN network configured"
          incus network list
          incus profile show default

      - name: Initialize Incus (Bridge)
        if: matrix.network_type == 'bridge'
        run: |
          # Wait for Incus service to be ready
          sudo systemctl start incus.socket || true
          sleep 5

          # Configure subuid/subgid for UID mapping (needed for raw.idmap)
          # Map runner UID 1001 to be available for container remapping
          echo "root:1001:1" | sudo tee -a /etc/subuid
          echo "root:1001:1" | sudo tee -a /etc/subgid

          # Restart Incus to pick up subuid/subgid changes
          sudo systemctl restart incus || true
          sleep 5

          # Initialize Incus with btrfs storage and standard bridge networking
          # NO OVN - this tests error handling for non-OVN networks
          cat <<EOF | sudo incus admin init --preseed
          config:
            images.compression_algorithm: none
          networks:
          - config:
              ipv4.address: 10.47.62.1/24
              ipv4.nat: "true"
              ipv6.address: none
            name: incusbr0
            type: bridge
          storage_pools:
          - config:
              size: 15GiB
            name: default
            driver: btrfs
          profiles:
          - config: {}
            devices:
              eth0:
                name: eth0
                network: incusbr0
                type: nic
              root:
                path: /
                pool: default
                type: disk
            name: default
          EOF

          # Allow access without re-login by changing socket permissions
          sudo chmod 666 /var/lib/incus/unix.socket

          # Add current user to incus-admin group
          sudo usermod -aG incus-admin $USER

          echo "Bridge network configured successfully"
          incus network list

      - name: Check kernel idmap support
        run: |
          echo "Kernel version:"
          uname -r
          echo "Checking for idmapped mount support:"
          grep -i idmap /proc/filesystems || echo "No idmap in /proc/filesystems"
          echo "Checking btrfs features:"
          sudo btrfs filesystem df /var/lib/incus/storage-pools/default || true
          echo "AppArmor status:"
          sudo aa-status | grep incus || echo "No incus AppArmor profiles"
          echo "Checking if AppArmor is blocking bind mounts:"
          sudo dmesg | grep -i "apparmor.*denied" | tail -20 || echo "No recent AppArmor denials"

      - name: Configure basic networking for Incus
        run: |
          # Enable IP forwarding (required for ALL container networking)
          echo 1 | sudo tee /proc/sys/net/ipv4/ip_forward > /dev/null
          echo 1 | sudo tee /proc/sys/net/ipv6/conf/all/forwarding > /dev/null

          # Get default interface for outbound traffic
          DEFAULT_IFACE=$(ip route | grep default | awk '{print $5}' | head -1)

          # Allow container traffic through firewall
          if sudo iptables -nL DOCKER-USER 2>/dev/null; then
            # Use DOCKER-USER chain if it exists
            sudo iptables -I DOCKER-USER -i incusbr0 -j ACCEPT
            sudo iptables -I DOCKER-USER -o incusbr0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
          else
            # Fallback to FORWARD chain
            sudo iptables -P FORWARD ACCEPT
            sudo iptables -I FORWARD 1 -i incusbr0 -j ACCEPT
            sudo iptables -I FORWARD 1 -o incusbr0 -j ACCEPT
          fi

          echo "Basic network configuration complete"

      - name: Configure OVN-specific networking
        if: matrix.network_type == 'ovn'
        run: |
          # Get default interface for outbound traffic
          DEFAULT_IFACE=$(ip route | grep default | awk '{print $5}' | head -1)

          # Allow traffic to/from OVN subnet through firewall
          # Allow traffic destined for OVN subnet (host-to-container)
          sudo iptables -I FORWARD 1 -d 10.215.220.0/24 -j ACCEPT
          # Allow return traffic from OVN subnet (container-to-host responses)
          sudo iptables -I FORWARD 1 -s 10.215.220.0/24 -j ACCEPT

          # Add NAT/MASQUERADE rules for OVN subnet (10.215.220.0/24)
          sudo iptables -t nat -I POSTROUTING 1 -s 10.215.220.0/24 -o $DEFAULT_IFACE -j MASQUERADE

          # Add route so host can reach OVN network (10.215.220.0/24) via OVN uplink IP
          # The OVN network uses 10.47.62.100 as its uplink IP on the bridge
          # This allows tests to curl container services from the host
          sudo ip route add 10.215.220.0/24 via 10.47.62.100 dev incusbr0 || true

          echo "OVN network configuration complete"

      - name: Cache Ubuntu base image
        id: cache-ubuntu-image
        uses: actions/cache@8b402f58fbc84540c8b491a91e594a4576fec3d7 # v5.0.2
        with:
          path: /tmp/ubuntu-24.04-image.tar.gz
          key: ${{ runner.os }}-ubuntu-24.04-image-v1

      - name: Restore Ubuntu image from cache
        if: steps.cache-ubuntu-image.outputs.cache-hit == 'true'
        run: |
          echo "Restoring Ubuntu 22.04 image from cache..."
          incus image import /tmp/ubuntu-24.04-image.tar.gz --alias ubuntu-24.04
          incus image list

      - name: Download Ubuntu image
        if: steps.cache-ubuntu-image.outputs.cache-hit != 'true'
        run: |
          echo "Downloading Ubuntu 22.04 base image directly from Ubuntu..."

          # Download rootfs and metadata from Ubuntu's cloud images
          # This bypasses the need for image servers (no dependency on images.linuxcontainers.org)
          cd /tmp

          # Download the latest Ubuntu 24.04 (noble) cloud image
          UBUNTU_VERSION="24.04"
          UBUNTU_CODENAME="noble"
          BASE_URL="https://cloud-images.ubuntu.com/releases/${UBUNTU_VERSION}/release"

          echo "Downloading Ubuntu ${UBUNTU_VERSION} rootfs..."
          curl -L -o ubuntu-rootfs.tar.xz "${BASE_URL}/ubuntu-${UBUNTU_VERSION}-server-cloudimg-amd64-root.tar.xz"

          echo "Creating metadata..."
          cat > metadata.yaml << EOF
          architecture: x86_64
          creation_date: $(date +%s)
          properties:
            description: Ubuntu ${UBUNTU_VERSION} (${UBUNTU_CODENAME})
            os: Ubuntu
            release: ${UBUNTU_CODENAME}
          EOF

          # Create metadata tarball
          tar -czf metadata.tar.gz metadata.yaml

          echo "Importing image into Incus..."
          incus image import metadata.tar.gz ubuntu-rootfs.tar.xz --alias ubuntu-24.04

          echo "Removing images: remote to prevent fallback to images.linuxcontainers.org..."
          incus remote remove images || true

          echo "✓ Successfully imported Ubuntu 24.04 image"
          incus image list

          # Export image for caching
          incus image export ubuntu-24.04 /tmp/ubuntu-24.04-image

          # Cleanup
          rm -f ubuntu-rootfs.tar.xz metadata.yaml metadata.tar.gz

      - name: Save Ubuntu image to cache
        if: steps.cache-ubuntu-image.outputs.cache-hit != 'true'
        uses: actions/cache/save@8b402f58fbc84540c8b491a91e594a4576fec3d7 # v5.0.2
        with:
          path: /tmp/ubuntu-24.04-image.tar.gz
          key: ${{ runner.os }}-ubuntu-24.04-image-v1

      - name: Build COI binary
        run: |
          go build -o coi ./cmd/coi
          ./coi version

      - name: Configure COI for CI (use open network mode for builds)
        run: |
          # Use open mode by default so image builds can access internet
          # Tests will explicitly use restricted/allowlist modes when testing network isolation
          mkdir -p ~/.config/coi
          cat > ~/.config/coi/config.toml << 'EOF'
          [network]
          mode = "open"
          EOF
          echo "Created COI config with network.mode = open (tests can override for isolation testing)"

      - name: Test bind mount functionality
        run: |
          echo "Testing bind mounts with UID mapping (using raw.idmap like COI does)..."

          # Create test directory and file
          mkdir -p /tmp/test-bind-mount
          echo "test-content" > /tmp/test-bind-mount/test-file.txt

          # Launch test container using pre-cached image
          incus launch ubuntu-24.04 test-bind-mount-container
          sleep 5

          # Configure UID mapping for CI environment (runner UID 1001 → container UID 0)
          # This is what COI does internally in CI environments
          incus config set test-bind-mount-container raw.idmap "both 1001 0"

          # Restart container to apply idmap changes
          incus restart test-bind-mount-container
          sleep 5

          # Add bind mount WITHOUT shift=true (using raw.idmap instead)
          incus config device add test-bind-mount-container testmount disk source=/tmp/test-bind-mount path=/mnt/test

          # Check if file is visible inside container
          echo "Files in container /mnt/test:"
          incus exec test-bind-mount-container -- ls -la /mnt/test/

          # Try to read the file
          echo "Reading file from container:"
          incus exec test-bind-mount-container -- cat /mnt/test/test-file.txt || echo "FAILED: Cannot read file in container"

          # Try to create a file from inside container
          incus exec test-bind-mount-container -- sh -c 'echo "created-from-container" > /mnt/test/created.txt'

          # Check if file appears on host
          if [ -f /tmp/test-bind-mount/created.txt ]; then
            echo "SUCCESS: File created in container is visible on host"
            cat /tmp/test-bind-mount/created.txt
          else
            echo "FAILED: File created in container NOT visible on host"
            ls -la /tmp/test-bind-mount/
          fi

          # Cleanup
          incus delete test-bind-mount-container --force
          rm -rf /tmp/test-bind-mount

      - name: Restore COI image from cache
        id: cache-coi-image
        uses: actions/cache/restore@8b402f58fbc84540c8b491a91e594a4576fec3d7 # v5.0.2
        with:
          path: /tmp/coi-image.tar.gz
          key: ${{ runner.os }}-coi-image-${{ hashFiles('internal/image/**', 'testdata/dummy/**', 'scripts/build/**') }}

      - name: Import COI image from cache
        if: steps.cache-coi-image.outputs.cache-hit == 'true'
        run: |
          echo "Restoring COI image from cache..."
          incus image import /tmp/coi-image.tar.gz --alias coi
          ./coi images

      - name: Build COI image
        if: steps.cache-coi-image.outputs.cache-hit != 'true'
        run: |
          echo "Building COI image (not cached)..."
          ./coi build
          ./coi images
          # Export image for caching
          incus image export coi /tmp/coi-image

      - name: Save COI image to cache
        if: steps.cache-coi-image.outputs.cache-hit != 'true'
        uses: actions/cache/save@8b402f58fbc84540c8b491a91e594a4576fec3d7 # v5.0.2
        with:
          path: /tmp/coi-image.tar.gz
          key: ${{ runner.os }}-coi-image-${{ hashFiles('internal/image/**', 'testdata/dummy/**', 'scripts/build/**') }}

      - name: Run ${{ matrix.test_group.name }} tests with coverage
        run: |
          echo "============================================"
          echo "Running: ${{ matrix.test_group.description }}"
          echo "Network: ${{ matrix.network_type }}"
          echo "Path: ${{ matrix.test_group.path }}"
          echo "============================================"

          # Run test group with coverage reporting
          python -m pytest ${{ matrix.test_group.path }} -v --tb=short --durations=0 --cov=tests --cov-report=term-missing
        env:
          COI_BINARY: ./coi
          CI_NETWORK_TYPE: ${{ matrix.network_type }}
          GITHUB_REPOSITORY_URL: ${{ github.event.pull_request.head.repo.clone_url || format('https://github.com/{0}.git', github.repository) }}

      - name: Cleanup
        if: always()
        run: |
          ./coi kill --all --force || true
          ./coi clean --force || true

  ci-success:
    name: CI Success
    runs-on: ubuntu-24.04
    timeout-minutes: 5
    if: always()
    needs:
      - build
      - unit-tests
      - integration
    steps:
      - name: Check all jobs passed
        if: |
          contains(needs.*.result, 'failure') ||
          contains(needs.*.result, 'cancelled') ||
          contains(needs.*.result, 'skipped')
        run: exit 1
      - run: echo "All CI checks passed!"
